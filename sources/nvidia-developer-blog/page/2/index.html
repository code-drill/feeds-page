<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Nvidia-Developer-Blog | Daily Tech Articles Feed</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'><text y='20' font-size='20'>📡</text></svg>">
    <style>
        body { 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; 
            margin: 0; 
            padding: 0; 
            background: #fff;
            color: #333;
            line-height: 1.5;
            font-size: 14px;
        }
        
         
        .header { 
            background: #f8f9fa; 
            border-bottom: 1px solid #e9ecef; 
            padding: 10px 0;
        }
        .header .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 0 20px;
        }
        .header h1 { 
            margin: 0; 
            font-size: 24px; 
            color: #333;
        }
        .header h1 a {
            color: #333;
            text-decoration: none;
        }
        .header h1 a:hover {
            color: #337ab7;
        }
        .nav { 
            margin-top: 10px;
        }
        .nav a { 
            color: #666; 
            text-decoration: none; 
            margin-right: 20px;
            font-size: 14px;
        }
        .nav a:hover { 
            color: #333; 
            text-decoration: underline;
        }
        
         
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px;
        }
        
         
        .articles-list { 
            margin-top: 20px;
        }
        
         
        .day-label {
            font-size: 18px;
            font-weight: bold;
            color: #333;
            margin: 40px 0 20px 0;
            padding: 12px 16px;
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-left: 4px solid #0066cc;
            border-radius: 4px;
        }
        .day-label:first-child {
            margin-top: 20px;
        }
        
        .article-item { 
            margin-bottom: 30px; 
            border-bottom: 1px solid #f0f0f0;
            padding-bottom: 20px;
        }
        .article-item:last-child { 
            border-bottom: none; 
        }
        .article-item h2 { 
            margin: 0 0 8px 0; 
            font-size: 18px; 
            line-height: 1.3;
        }
        .article-item h2 a { 
            color: #337ab7; 
            text-decoration: none; 
        }
        .article-item h2 a:hover { 
            color: #286090; 
            text-decoration: underline;
        }
        
         
        .publication-date { 
            font-size: 13px; 
            color: #666; 
            margin-bottom: 15px;
        }
        .source { 
            font-size: 13px; 
            color: #337ab7; 
            margin-bottom: 6px;
        }
        .source a {
            color: #337ab7;
            text-decoration: none;
        }
        .source a:hover {
            color: #286090;
        }
        .author { 
            font-size: 13px; 
            color: #666; 
            margin-bottom: 6px;
        }
        .author::before {
            content: "Author: ";
        }
        
         
        .summary { 
            color: #555; 
            line-height: 1.4; 
            margin-bottom: 8px;
        }
        
         
        .categories { 
            margin-bottom: 8px;
        }
        .categories::before {
            content: "Category: ";
            font-size: 13px;
            color: #666;
        }
        .category-text { 
            font-size: 13px; 
            color: #337ab7;
            text-decoration: none;
        }
        .category-text:hover { 
            color: #286090;
        }
        .category { 
            font-size: 13px; 
            color: #337ab7;
            text-decoration: none;
        }
        .category:hover { 
            color: #286090;
        }
        
         
        .pagination-info { 
            text-align: center; 
            margin: 20px 0; 
            color: #666; 
            font-size: 14px;
        }
        .pagination { 
            display: flex; 
            justify-content: center; 
            align-items: center; 
            gap: 10px; 
            margin: 30px 0; 
            flex-wrap: wrap;
        }
        .pagination-numbers { 
            display: flex; 
            gap: 5px;
        }
        .pagination-link { 
            display: inline-block; 
            padding: 6px 10px; 
            border: 1px solid #ddd; 
            text-decoration: none; 
            color: #333; 
            background: #fff;
        }
        .pagination-link:hover { 
            background: #f0f0f0; 
            border-color: #ccc;
        }
        .pagination-current { 
            display: inline-block; 
            padding: 6px 10px; 
            background: #333; 
            color: white; 
            font-weight: bold;
        }
        
         
        .filters-container { 
            margin-bottom: 25px; 
            padding: 20px;
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
        }
        .filter-section { 
            margin-bottom: 18px;
        }
        .filter-section:last-child {
            margin-bottom: 0;
        }
        .filter-section h3 { 
            margin: 0 0 12px 0; 
            color: #333; 
            font-size: 16px;
            padding-bottom: 6px;
            border-bottom: 2px solid #0066cc;
            display: inline-block;
        }
        .filter-options { 
            display: flex; 
            flex-wrap: wrap; 
            gap: 8px;
        }
        .filter-link { 
            display: inline-block; 
            padding: 6px 12px; 
            background: transparent; 
            color: #337ab7; 
            text-decoration: none; 
            font-size: 13px;
            border: none;
            border-radius: 20px;
            transition: all 0.2s ease;
        }
        .filter-link:hover { 
            background: #286090;
            color: white;
        }
        .filter-link.all-link { 
            background: transparent; 
            color: #337ab7;
            font-weight: bold;
        }
        .filter-link.all-link:hover { 
            background: #286090;
            color: white;
        }
        
         
        .filter-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            cursor: pointer;
            margin-bottom: 12px;
        }
        .filter-toggle {
            background: none;
            border: none;
            font-size: 18px;
            color: #337ab7;
            cursor: pointer;
            padding: 0;
            margin: 0;
            line-height: 1;
            transition: transform 0.2s ease;
        }
        .filter-toggle:hover {
            color: #286090;
        }
        .filter-content {
            overflow: hidden;
            transition: max-height 0.3s ease;
        }
        .filter-content.collapsed {
            max-height: 0;
        }
        .filter-content.expanded {
            max-height: 500px;
        }
        .show-more-btn {
            display: inline-block;
            padding: 4px 8px;
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            color: #337ab7;
            text-decoration: none;
            font-size: 12px;
            border-radius: 4px;
            margin-top: 8px;
            cursor: pointer;
        }
        .show-more-btn:hover {
            background: #e9ecef;
            color: #286090;
        }
        
         
        .filter-input {
            width: 100%;
            padding: 8px 12px;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            font-size: 14px;
            margin-bottom: 12px;
            background: white;
            color: #333;
        }
        .filter-input:focus {
            outline: none;
            border-color: #337ab7;
            box-shadow: 0 0 0 2px rgba(51, 122, 183, 0.1);
        }
        .filter-input::placeholder {
            color: #999;
            font-style: italic;
        }
        
         
        .breadcrumb { margin-bottom: 20px; }
        .breadcrumb a { color: #337ab7; text-decoration: none; }
        .breadcrumb a:hover { color: #286090; }
        
         
        .taxonomy-index { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px; }
        .taxonomy-item { padding: 15px; border: 1px solid #eee; border-radius: 8px; background: #fafafa; }
        .taxonomy-item h3 { margin: 0 0 5px 0; display: flex; align-items: center; gap: 8px; }
        .taxonomy-item .count { color: #666; font-size: 14px; font-weight: normal; }
        .taxonomy-item .taxonomy-description { color: #666; font-size: 14px; margin: 0; }
        .taxonomy-item h3 a { 
            color: #337ab7; 
            text-decoration: none; 
        }
        .taxonomy-item h3 a:hover { 
            color: #286090; 
            text-decoration: underline;
        }
        
         
        .site-footer {
            margin-top: 40px;
            padding: 20px 0;
            border-top: 1px solid #e9ecef;
            background: #f8f9fa;
        }
        .footer-text {
            text-align: center;
            margin: 0;
            font-size: 13px;
            color: #666;
        }
        .author-link {
            color: #337ab7;
            text-decoration: none;
        }
        .author-link:hover {
            color: #286090;
            text-decoration: underline;
        }
        
        @media (max-width: 768px) {
            .container { 
                padding: 15px;
            }
            
             
            .filters-container {
                padding: 15px;
                margin-bottom: 20px;
            }
            .filter-section {
                margin-bottom: 15px;
            }
            .filter-section h3 {
                font-size: 15px;
                margin: 0;
                border: none;
                padding: 0;
            }
            .filter-content {
                margin-top: 8px;
            }
            .filter-content.collapsed {
                max-height: 0;
                margin-top: 0;
            }
            .filter-options { 
                justify-content: flex-start;
                gap: 6px;
            }
            .filter-link {
                padding: 4px 8px;
                font-size: 11px;
                border-radius: 12px;
                min-height: 36px;
                display: flex;
                align-items: center;
                line-height: 1.2;
            }
            
             
            .filter-content {
                max-height: 0;
            }
            .filter-content.expanded {
                max-height: none;
                overflow-y: auto;
                max-height: 60vh;
            }
            
             
            .filter-input {
                font-size: 13px;
                padding: 6px 10px;
                margin-bottom: 10px;
            }
            
            .pagination { 
                flex-direction: column; 
                gap: 10px;
            }
            .pagination-numbers { 
                flex-wrap: wrap; 
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    
<div class="container">
    <h1>Articles from Source: Nvidia-Developer-Blog</h1>
    
    <nav class="breadcrumb">
        <a href="/">← Back to all articles</a>
    </nav>
    
    
    <div class="articles-list">
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/new-open-source-qwen3-next-models-preview-hybrid-moe-architecture-delivering-improved-accuracy-and-accelerated-parallel-processing-across-nvidia-platform/" target="_blank">New Open Source Qwen3-Next Models Preview Hybrid MoE Architecture Delivering Improved Accuracy and Accelerated Parallel Processing across NVIDIA Platform</a></h2>
            <div class="publication-date">2025-09-15 13:00</div>
            <div class="summary">🚀 Alibaba has unveiled two new open-source models: Qwen3-Next 80B-A3B-Thinking and Qwen3-Next 80B-A3B-Instruct. These models feature a hybrid Mixture of Experts (MoE) architecture designed for improved efficiency and accuracy. 🔍 The Qwen3-Next-80B-A3B-Thinking model is now available on build.nvidia.com, allowing developers to explore its advanced reasoning capabilities. 💡 With 80 billion parameters, only a fraction is activated per token, optimizing processing for longer context lengths. The...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Anu Srivastava</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/modeling-attacks-on-ai-powered-apps-with-the-ai-kill-chain-framework/" target="_blank">Modeling Attacks on AI-Powered Apps with the AI Kill Chain Framework</a></h2>
            <div class="publication-date">2025-09-11 16:00</div>
            <div class="summary">AI-powered applications are facing new security challenges that traditional models may not address. The AI Kill Chain framework, developed by NVIDIA, outlines how adversaries target these systems. This framework emphasizes the stages of an attack: recon, poison, hijack, persist, and impact. It aims to help defenders identify where they can intervene effectively. Learn more about the evolving landscape of AI security! 🔐💻🛡️ #AI #CyberSecurity #NVIDIA #AIKillChain #TechTrends</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Rich Harang</div>
            
            
            <div class="categories">
                
                <a href="/categories/security_compliance" class="category-text">Security Compliance</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-quantization-aware-training-enables-low-precision-accuracy-recovery/" target="_blank">How Quantization Aware Training Enables Low-Precision Accuracy Recovery</a></h2>
            <div class="publication-date">2025-09-11 15:00</div>
            <div class="summary">Optimizing AI models for deployment involves various compression techniques. Post-training quantization (PTQ) is common, but quantization aware training (QAT) and quantization aware distillation (QAD) provide significant advantages. These methods prepare models for lower precision by simulating quantization effects, enhancing accuracy recovery. Learn more about these techniques and their impact on model performance! 📊🤖 #AI #Quantization #MachineLearning #ModelOptimization #TechTrends</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Eduardo Alvarez</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/accelerate-protein-structure-inference-over-100x-with-nvidia-rtx-pro-6000-blackwell-server-edition/" target="_blank">Accelerate Protein Structure Inference Over 100x with NVIDIA RTX PRO 6000 Blackwell Server Edition</a></h2>
            <div class="publication-date">2025-09-10 16:48</div>
            <div class="summary">Unlocking the future of protein structure analysis is now possible with the NVIDIA RTX PRO 6000 Blackwell Server Edition. This new GPU significantly accelerates protein structure inference, enhancing research efficiency and reducing costs for organizations. 🧬💻 With advancements from NVIDIA&#39;s Digital Biology Research labs, researchers can now utilize OpenFold for rapid analysis without sacrificing accuracy compared to AlphaFold2. Discover how this technology can transform large-scale protein...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Kyle Tretina</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/deploy-scalable-ai-inference-with-nvidia-nim-operator-3-0-0/" target="_blank">Deploy Scalable AI Inference with NVIDIA NIM Operator 3.0.0</a></h2>
            <div class="publication-date">2025-09-10 16:30</div>
            <div class="summary">Unlock the potential of AI with NVIDIA NIM Operator 3.0.0! 🚀 This latest release enhances the deployment of NVIDIA NIM and NeMo microservices in Kubernetes environments, making it easier to manage complex AI inference pipelines. Key features include efficient resource utilization and seamless integration with existing infrastructures, including KServe. 🤖 Collaboration with Red Hat further streamlines NIM deployment, supporting model caching and trusted AI capabilities. #NVIDIA #AI #Kubernetes...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Meenakshi Kaushik</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/developers-can-now-get-cuda-directly-from-their-favorite-third-party-platforms/" target="_blank">Developers Can Now Get CUDA Directly from Their Favorite Third-Party Platforms</a></h2>
            <div class="publication-date">2025-09-10 16:00</div>
            <div class="summary">🚀 Developers can now access CUDA directly through popular third-party platforms, making application deployment easier. NVIDIA is collaborating with Canonical, CIQ, and others to simplify installation and maintain compatibility across various OS and package managers. This initiative helps streamline the integration of GPU support in applications like PyTorch and OpenCV. Key benefits include consistent CUDA naming, timely updates, and continued free access to CUDA. #NVIDIA #CUDA #DeveloperTools...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Jonathan Bentz</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/maximizing-low-latency-networking-performance-for-financial-services-with-nvidia-rivermax-and-neio-fastsocket/" target="_blank">Maximizing Low-Latency Networking Performance for Financial Services with NVIDIA Rivermax and NEIO FastSocket</a></h2>
            <div class="publication-date">2025-09-10 16:00</div>
            <div class="summary">Ultra-low latency and reliable packet delivery are essential in sectors like financial services, cloud gaming, and media. Delays or packet losses can lead to significant issues, including financial losses and poor user experiences. NVIDIA Rivermax offers a high-performance solution for these challenges. It utilizes GPU-accelerated technologies to ensure high throughput, low latency, and minimal CPU usage, making it ideal for demanding applications. Learn more about how Rivermax is...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Simon Raviv</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-connect-distributed-data-centers-into-large-ai-factories-with-scale-across-networking/" target="_blank">How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking</a></h2>
            <div class="publication-date">2025-09-09 17:00</div>
            <div class="summary">AI scaling faces challenges due to physical limitations in data centers, such as power and cooling capacity. 🌐 Traditional long-haul Ethernet solutions can lead to high latency and unpredictable data delivery, which is problematic for AI workloads. NVIDIA&#39;s Spectrum-XGS Ethernet technology introduces scale-across networking, allowing multiple data centers to function as one large AI factory, enhancing performance for training and inference tasks. 🚀 #ArtificialIntelligence #DataCenters...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Taylor Allison</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/nvidia-blackwell-ultra-sets-new-inference-records-in-mlperf-debut/" target="_blank">NVIDIA Blackwell Ultra Sets New Inference Records in MLPerf Debut</a></h2>
            <div class="publication-date">2025-09-09 15:00</div>
            <div class="summary">🚀 NVIDIA&#39;s Blackwell Ultra architecture has made a significant impact in the latest MLPerf Inference v5.1 benchmarks. New models like DeepSeek-R1 and Llama 3.1 have set high performance standards, with impressive token processing speeds. The benchmarks highlight the growing need for advanced compute power as large language models evolve. NVIDIA continues to lead with record-breaking results across all tested scenarios. #NVIDIA #MLPerf #AI #MachineLearning #TechNews</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Ashwin Nanjappa</div>
            
            
            <div class="categories">
                
                <a href="/categories/industry_analysis" class="category-text">Industry Analysis</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/nvidia-rubin-cpx-accelerates-inference-performance-and-efficiency-for-1m-token-context-workloads/" target="_blank">NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M&#43; Token Context Workloads</a></h2>
            <div class="publication-date">2025-09-09 15:00</div>
            <div class="summary">NVIDIA is addressing the increasing complexity of AI inference with its new Rubin CPX GPU. This technology supports workloads requiring extensive context, like software development and long-form video generation. The NVIDIA SMART framework optimizes inference across various dimensions, allowing for better resource allocation. This disaggregated approach separates the context and generation phases, improving efficiency and reducing latency. Discover how NVIDIA is redefining AI infrastructure....</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Joe DeLaere</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-build-ai-systems-in-house-with-outerbounds-and-dgx-cloud-lepton/" target="_blank">How to Build AI Systems In House with Outerbounds and DGX Cloud Lepton</a></h2>
            <div class="publication-date">2025-09-08 16:00</div>
            <div class="summary">Building production-grade AI systems involves managing numerous components. Companies are increasingly opting to develop in-house solutions for better security and compliance. Outerbounds offers a cloud-native platform that simplifies this process, utilizing open-source Metaflow for efficient orchestration. Key to success is leveraging NVIDIA DGX Cloud Lepton for GPU access, enabling scalable AI operations. Explore how to create customized AI products while navigating the complex GPU cloud...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Ville Tuulos</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://nvda.ws/4g1w81S" target="_blank">Register for the Global Webinar: How to Prepare for NVIDIA Generative AI Certification</a></h2>
            <div class="publication-date">2025-09-07 15:00</div>
            <div class="summary">🌍 Join the global webinar on October 7 to learn how to prepare for the NVIDIA Generative AI Certification exams. Get insights into the new professional level certification and tips for success. Don&#39;t miss this opportunity to enhance your skills! #NVIDIA #GenerativeAI #Webinar #Certification #ProfessionalDevelopment</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Shara Tibken</div>
            
            
            <div class="categories">
                
                <a href="/categories/event" class="category-text">Event</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://www.nvidia.com/en-us/events/speech-ai-day/?nvid=nv-int-tblg-799133" target="_blank">Just Released: NVIDIA PhysicsNeMo 25.08</a></h2>
            <div class="publication-date">2025-09-05 17:37</div>
            <div class="summary">🚀 Exciting news from NVIDIA! The latest release of PhysicsNeMo 25.08 introduces powerful workflows and recipes specifically designed for CAE application developers. This update aims to enhance simulations and streamline development processes. Explore the new features and boost your CAE projects with NVIDIA&#39;s advanced tools! #NVIDIA #PhysicsNeMo #CAE #TechUpdates #Simulation</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Bhoomi Gadhia</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://www.nvidia.com/en-us/events/speech-ai-day/?nvid=nv-int-tblg-799133" target="_blank">Just Released: NVIDIA PhysicsNeMo 25.08</a></h2>
            <div class="publication-date">2025-09-05 17:37</div>
            <div class="summary">🚀 Exciting news for CAE developers! NVIDIA has just launched PhysicsNeMo 25.08, introducing new workflows and recipes designed to enhance application development. This update aims to streamline processes and improve efficiency in computational physics. Stay tuned for more advancements in simulation technology! #NVIDIA #PhysicsNeMo #CAE #TechUpdate #Simulation</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Bhoomi Gadhia</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/accelerate-large-scale-llm-inference-and-kv-cache-offload-with-cpu-gpu-memory-sharing/" target="_blank">Accelerate Large-Scale LLM Inference and KV Cache Offload with CPU-GPU Memory Sharing</a></h2>
            <div class="publication-date">2025-09-05 17:24</div>
            <div class="summary">Large Language Models (LLMs) like Llama 3 70B and Llama 4 Scout 109B are pushing AI boundaries but pose memory challenges for inference efficiency. These models can require significant memory, with Llama 3 needing around 140 GB and Llama 4 about 218 GB. The key-value (KV) cache also demands additional memory as context and batch sizes increase. NVIDIA&#39;s Grace Hopper and Blackwell architectures use NVLink-C2C, allowing CPU-GPU memory sharing. This innovation enhances data access and...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Afroze Syed</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/accelerate-large-scale-llm-inference-and-kv-cache-offload-with-cpu-gpu-memory-sharing/" target="_blank">Accelerate Large-Scale LLM Inference and KV Cache Offload with CPU-GPU Memory Sharing</a></h2>
            <div class="publication-date">2025-09-05 17:24</div>
            <div class="summary">Large Language Models (LLMs) like Llama 3 70B and Llama 4 Scout 109B face challenges with inference due to their size. These models can require significant memory, often exceeding GPU limits, especially with large context windows. The NVIDIA Grace architectures address this by utilizing NVLink C2C, allowing CPU and GPU to share memory efficiently. This setup enhances the processing of large datasets and enables quicker access, minimizing the risk of out-of-memory errors during inference....</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Afroze Syed</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/accelerate-large-scale-llm-inference-and-kv-cache-offload-with-cpu-gpu-memory-sharing/" target="_blank">Accelerate Large-Scale LLM Inference and KV Cache Offload with CPU-GPU Memory Sharing</a></h2>
            <div class="publication-date">2025-09-05 17:24</div>
            <div class="summary">Large Language Models (LLMs) like Llama 3 and Llama 4 are pushing AI boundaries, but their size poses challenges for inference efficiency. These models can require substantial GPU memory, often leading to out-of-memory errors during inference. The NVIDIA Grace architectures address this with NVLink C2C, offering a high-bandwidth connection that shares CPU and GPU memory. This innovation enhances processing capabilities, making it easier to handle large datasets and models. #AI #NVIDIA...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Afroze Syed</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/accelerate-autonomous-vehicle-development-with-the-nvidia-drive-agx-thor-developer-kit/" target="_blank">Accelerate Autonomous Vehicle Development with the NVIDIA DRIVE AGX Thor Developer Kit</a></h2>
            <div class="publication-date">2025-09-03 17:30</div>
            <div class="summary">🚗🔍 The NVIDIA DRIVE AGX Thor Developer Kit is now available, enhancing the development of autonomous vehicle technology. This platform supports advanced AI models for better perception and decision-making, enabling a comprehensive in-vehicle experience. With powerful Blackwell GPUs and next-gen Arm CPUs, it meets high safety and security standards. The DRIVE AGX Thor is designed to empower automotive OEMs and developers in scaling performance and efficiency for future demands. #NVIDIA...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Abhinaw Priyadershi</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-run-ai-powered-cae-simulations/" target="_blank">How to Run AI-Powered CAE Simulations</a></h2>
            <div class="publication-date">2025-09-03 16:09</div>
            <div class="summary">🚀 In modern engineering, accelerated simulations are crucial for innovation. Computer-aided engineering (CAE) helps design reliable products by verifying performance and safety. Traditional simulations take time, often hindering exploration of design options. Physics-based AI models serve as surrogates, predicting outcomes in seconds or minutes, thus enhancing the design process. This article outlines a modular workflow for automotive aerodynamics, leveraging NVIDIA technologies. It covers...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Abouzar Ghasemi</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/north-south-networks-the-key-to-faster-enterprise-ai-workloads/" target="_blank">North–South Networks: The Key to Faster Enterprise AI Workloads</a></h2>
            <div class="publication-date">2025-09-03 15:04</div>
            <div class="summary">In the realm of AI infrastructure, data movement is crucial for performance. As enterprises adopt advanced AI systems, they face challenges in quickly and reliably moving data. NVIDIA’s Enterprise Reference Architectures (RAs) provide guidance on optimizing north-south networks, essential for tasks like model loading and inference queries. By utilizing NVIDIA Spectrum-X Ethernet, organizations can enhance data flow, particularly for data-intensive AI applications. Legacy networks often...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Shashank Sabhlok</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/cut-model-deployment-costs-while-keeping-performance-with-gpu-memory-swap/" target="_blank">Cut Model Deployment Costs While Keeping Performance With GPU Memory Swap</a></h2>
            <div class="publication-date">2025-09-02 18:44</div>
            <div class="summary">Deploying large language models (LLMs) at scale involves balancing fast responsiveness and GPU costs. Organizations often face tough choices: over-provisioning GPUs or risking user experience with latency spikes. NVIDIA&#39;s GPU memory swap, or model hot-swapping, offers a solution. This innovation allows multiple models to share GPUs, dynamically offloading inactive models to CPU memory, enabling rapid activation when needed. Benchmark tests show promising results with lower costs and improved...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Ekin Karabulut</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/improving-gemm-kernel-auto-tuning-efficiency-on-nvidia-gpus-with-heuristics-and-cutlass-4-2/" target="_blank">Improving GEMM Kernel Auto-Tuning Efficiency on NVIDIA GPUs with Heuristics and CUTLASS 4.2</a></h2>
            <div class="publication-date">2025-09-02 17:00</div>
            <div class="summary">🚀 Selecting the optimal GEMM kernel for specific hardware is challenging due to the many performance-determining parameters. NVIDIA introduces **nvMatmulHeuristics** to enhance the process. This module identifies a small set of top-performing kernel configurations, simplifying the tuning workflow and saving time. ⏱️ With nvMatmulHeuristics and CUTLASS 4.2, users can quickly generate and auto-tune kernels, leading to faster model compilation and better performance. #NVIDIA #GEMM #CUDA...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Harrison Barclay</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/whats-new-in-cuda-toolkit-13-0-for-jetson-thor-unified-arm-ecosystem-and-more/" target="_blank">What’s New in CUDA Toolkit 13.0 for Jetson Thor: Unified Arm Ecosystem and More</a></h2>
            <div class="publication-date">2025-09-02 16:00</div>
            <div class="summary">🚀 Exciting advancements are on the horizon with CUDA Toolkit 13.0 for Jetson Thor! This release introduces a unified toolkit for Arm platforms, eliminating the need for separate installations. Developers can build applications once and deploy them seamlessly across various systems. Enhanced features like Unified Virtual Memory and improved developer tools streamline workflows and enhance performance for edge AI applications. #NVIDIA #CUDA #JetsonThor #EdgeComputing #AI</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Rekha Mukund</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-small-language-models-are-key-to-scalable-agentic-ai/" target="_blank">How Small Language Models Are Key to Scalable Agentic AI</a></h2>
            <div class="publication-date">2025-08-29 18:00</div>
            <div class="summary">The rise of agentic AI is transforming how businesses approach automation and productivity. 🤖 Recent insights highlight the potential of small language models (SLMs) as efficient alternatives to large language models (LLMs) in agentic applications. SLMs can reduce costs and improve operational flexibility while maintaining performance. This shift enables enterprises to utilize SLMs for specific tasks, reserving LLMs for more complex scenarios. Tools like NVIDIA’s Nemotron demonstrate the...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Peter Belcak</div>
            
            
            <div class="categories">
                
                <a href="/categories/industry_analysis" class="category-text">Industry Analysis</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/" target="_blank">Fine-Tuning gpt-oss for Accuracy and Performance with Quantization Aware Training</a></h2>
            <div class="publication-date">2025-08-29 14:47</div>
            <div class="summary">OpenAI&#39;s gpt-oss model has made waves in the AI community with its innovative architecture and performance capabilities. 📈🧠 It features a mixture of expert architecture and a 128K context length, competing closely with OpenAI&#39;s closed-source models. However, deploying foundational models like gpt-oss in critical fields requires careful fine-tuning. The article discusses employing Supervised Fine-Tuning (SFT) and Quantization-Aware Training (QAT) to enhance model accuracy while maintaining...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Eduardo Alvarez</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-isaac-for-healthcare-using-the-telesurgery-workflow/" target="_blank">Getting Started with NVIDIA Isaac for Healthcare Using the Telesurgery Workflow</a></h2>
            <div class="publication-date">2025-08-28 16:00</div>
            <div class="summary">🚀 Telesurgery is transforming healthcare delivery as the shortage of surgeons rises. With advancements in 5G and AI, experts can now operate remotely, shifting from experimental to essential. 🌍 NVIDIA Isaac for Healthcare offers a modular workflow that includes video streaming, robot control, and simulation tools. This enables seamless training and clinical deployment. Learn how this technology is paving the way for the next generation of surgical robotics. 🤖💡 #Telesurgery...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Michael Zephyr</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-improve-cuda-kernel-performance-with-shared-memory-register-spilling/" target="_blank">How to Improve CUDA Kernel Performance with Shared Memory Register Spilling</a></h2>
            <div class="publication-date">2025-08-27 16:30</div>
            <div class="summary">🚀 New in CUDA Toolkit 13.0: Shared Memory Register Spilling! This feature helps improve CUDA kernel performance by allowing the compiler to use shared memory for excess variables instead of local memory. This reduces spill latency and L2 pressure for register-heavy kernels. To enable shared memory spilling, use the pragma command in your kernel definition. With this optimization, kernels can perform better, especially in critical regions where registers are heavily used. Learn more about how...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Divya Shanmughan</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-scale-your-langgraph-agents-in-production-from-a-single-user-to-1000-coworkers/" target="_blank">How to Scale Your LangGraph Agents in Production From A Single User to 1,000 Coworkers</a></h2>
            <div class="publication-date">2025-08-27 16:00</div>
            <div class="summary">📈 Scaling your AI agent for production use? In a recent article, the deployment of a deep-research agent using the AI-Q NVIDIA Blueprint is explored. This article outlines how NVIDIA tackled the challenges of sharing their AI tools with up to 1,000 coworkers. The focus was on using the NeMo Agent Toolkit to ensure scalability and security while accessing internal data. It details the architecture that supports document processing and web search capabilities. Learn more about the techniques...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Sean Lopp</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-industry-collaboration-fosters-nvidia-co-packaged-optics/" target="_blank">How Industry Collaboration Fosters NVIDIA Co-Packaged Optics</a></h2>
            <div class="publication-date">2025-08-26 17:00</div>
            <div class="summary">NVIDIA is transforming data-center connectivity by merging optical and electrical components through strong industry partnerships. 🤝 Their networking platform integrates advanced technologies from top partners, focusing on scalable and efficient optical systems. Key innovations include the Micro Ring Modulator, allowing high data throughput with a compact design. Collaboration with TSMC has addressed manufacturing challenges, ensuring reliable performance essential for modern data centers....</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Ashkan Seyedi</div>
            
            
            <div class="categories">
                
                <a href="/categories/industry_analysis" class="category-text">Industry Analysis</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/nvfp4-trains-with-precision-of-16-bit-and-speed-and-efficiency-of-4-bit/" target="_blank">NVFP4 Trains with Precision of 16-Bit and Speed and Efficiency of 4-Bit</a></h2>
            <div class="publication-date">2025-08-25 17:59</div>
            <div class="summary">🚀 NVIDIA has introduced NVFP4, a 4-bit format designed to enhance AI workloads during pretraining of large language models (LLMs). This innovation aims to improve training efficiency and throughput while maintaining accuracy. The shift from higher precision formats to 4-bit is set to redefine scalability in AI development. Collaboration with major organizations like Google Cloud and OpenAI is ongoing to explore this technology&#39;s full potential. #AI #NVIDIA #MachineLearning #LLMs #Innovation</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Kirthi Devleker</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/introducing-nvidia-jetson-thor-the-ultimate-platform-for-physical-ai/" target="_blank">Introducing NVIDIA Jetson Thor, the Ultimate Platform for Physical AI</a></h2>
            <div class="publication-date">2025-08-25 17:57</div>
            <div class="summary">🚀 Robotics is evolving! The shift from specialist machines to adaptable robots marks a new era in generalist robotics. These robots are designed to learn and perform various tasks, enhancing efficiency across industries. With NVIDIA&#39;s Jetson Thor platform, developers can create flexible robots that streamline operations without constant reprogramming. Key components include hardware integration, real-time control, perception, and high-level reasoning to facilitate complex interactions....</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Shashank Maheshwari</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/how-to-spot-and-fix-5-common-performance-bottlenecks-in-pandas-workflows/" target="_blank">How to Spot (and Fix) 5 Common Performance Bottlenecks in pandas Workflows</a></h2>
            <div class="publication-date">2025-08-22 19:54</div>
            <div class="summary">Are you facing slow data loads and memory issues in your pandas workflows? 🐍💻 This article highlights five common performance bottlenecks in pandas, including slow CSV parsing and memory-intensive joins. It offers practical solutions to improve your workflow efficiency, such as using the PyArrow engine for faster CSV reads and exploring the cudf.pandas library for GPU acceleration. Don&#39;t have a GPU? You can use cudf.pandas for free in Google Colab! 🚀📊 #DataScience #Python #Pandas #Performance...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Jamil Semaan</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/" target="_blank">Inside NVIDIA Blackwell Ultra: The Chip Powering the AI Factory Era</a></h2>
            <div class="publication-date">2025-08-22 17:58</div>
            <div class="summary">Introducing the NVIDIA Blackwell Ultra GPU, a key advancement in the Blackwell architecture. This GPU enhances AI training and reasoning with innovative technology. Key features include a dual-reticle design, high bandwidth, and energy-efficient performance. It boasts 208 billion transistors and provides significant scalability for AI tasks. With 15 PetaFLOPS performance and improved memory access, the Blackwell Ultra sets a new standard for accelerated computing. #NVIDIA #AI #BlackwellUltra...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Kyle Aubrey</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/nvidia-hardware-innovations-and-open-source-contributions-are-shaping-ai/" target="_blank">NVIDIA Hardware Innovations and Open Source Contributions Are Shaping AI</a></h2>
            <div class="publication-date">2025-08-22 15:00</div>
            <div class="summary">NVIDIA is making strides in AI through open source models like Cosmos, DeepSeek, and Llama. 🌐 These models offer free access to AI methodologies, enabling innovation across the globe. Their new Blackwell GPU architecture enhances AI performance with advanced features like NVFP4 and high-bandwidth interconnects. ⚡️ Additionally, NVIDIA provides a wealth of open source tools and libraries, fostering an environment for developers to build and scale AI efficiently. 💻 Discover more about these...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">George Chellapa</div>
            
            
            <div class="categories">
                
                <a href="/categories/industry_analysis" class="category-text">Industry Analysis</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/less-coding-more-science-simplify-ocean-modeling-on-gpus-with-openacc-and-unified-memory/" target="_blank">Less Coding, More Science: Simplify Ocean Modeling on GPUs With OpenACC and Unified Memory</a></h2>
            <div class="publication-date">2025-08-21 16:53</div>
            <div class="summary">🚀 Exciting advancements in ocean modeling are here! NVIDIA HPC SDK v25.7 simplifies GPU programming for high-performance computing applications. This update automates data movement between CPU and GPU, reducing manual management and enhancing developer productivity. Notable systems like the NVIDIA GH200 Grace Hopper Superchip are leading the way. With unified memory programming, developers can focus more on science and less on coding complexities. This change is already benefiting projects,...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Anastasia Stulova</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/improve-data-integrity-and-security-with-accelerated-hash-functions-and-merkle-trees-in-cupqc-0-4/" target="_blank">Improve Data Integrity and Security with Accelerated Hash Functions and Merkle Trees in cuPQC 0.4</a></h2>
            <div class="publication-date">2025-08-21 15:00</div>
            <div class="summary">🔒 As data sizes grow, ensuring security and integrity is vital. The cuPQC SDK v0.4 offers advanced cryptographic techniques, including inclusion proofs and digital signatures, to enhance data protection. New features include expanded hash function support and efficient Merkle tree calculations, improving performance in data verification. 🌳 Discover how these updates can benefit your cryptographic tasks! #DataIntegrity #Cryptography #cuPQC #MerkleTrees #CyberSecurity</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Yarkin Doroz</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/scaling-ai-inference-performance-and-flexibility-with-nvidia-nvlink-and-nvlink-fusion/" target="_blank">Scaling AI Inference Performance and Flexibility with NVIDIA NVLink and NVLink Fusion</a></h2>
            <div class="publication-date">2025-08-21 15:00</div>
            <div class="summary">The rise of AI model complexity has increased parameter counts from millions to trillions, demanding more computational power. 🌐 NVIDIA NVLink and NVLink Fusion are key technologies enhancing AI inference performance. They enable large-scale parallelization strategies, essential for handling advanced AI architectures like mixture-of-experts (MoE). 🤖 This evolution in AI systems highlights the need for interconnected GPUs acting as a unified pool of compute and memory. #AI #NVIDIA #NVLink...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Joe DeLaere</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/reinforcement-learning-with-nvidia-nemo-rl-megatron-core-support-for-optimized-training-throughput/" target="_blank">Reinforcement Learning with NVIDIA NeMo-RL: Megatron-Core Support for Optimized Training Throughput</a></h2>
            <div class="publication-date">2025-08-20 15:15</div>
            <div class="summary">🚀 Exciting updates in reinforcement learning with NVIDIA NeMo-RL! The latest release introduces support for the Megatron-Core library, enhancing training throughput for massive language models. This integration addresses limitations found in the PyTorch DTensor backend, particularly for models with hundreds of billions of parameters. With GPU-optimized techniques and simplified configuration options, NeMo-RL makes it easier for developers to harness the power of Megatron-Core. Explore...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Anna Shors</div>
            
            
            <div class="categories">
                
                <a href="/categories/technical_deep_dives" class="category-text">Technical Deep Dives</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://developer.nvidia.com/blog/deploying-your-omniverse-kit-apps-at-scale/" target="_blank">Deploying Your Omniverse Kit Apps at Scale</a></h2>
            <div class="publication-date">2025-08-20 13:00</div>
            <div class="summary">Unlock the potential of 3D applications with NVIDIA Omniverse Kit App Streaming! 🌐 This solution simplifies deployment and enables users to stream applications directly from their browsers, reducing the need for complex installations. With flexible options like self-managed deployment or fully-managed infrastructure, developers can easily reach their audience. Explore the straightforward steps to get started and enhance your 3D application experience! 💻🚀 #NVIDIA #Omniverse #3DStreaming...</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Ashley Goldstein</div>
            
            
            <div class="categories">
                
                <a href="/categories/educational" class="category-text">Educational</a>
                
            </div>
            
        </article>
        
        <article class="article-item">
            <h2><a href="https://huggingface.co/blog/nvidia/supercharge-ai-reasoning-with-nemotron-nano-2?nvid=nv-int-tblg-513492%20" target="_blank">New Nemotron Nano 2 Open Reasoning Model Tops Leaderboard and Delivers 6x Higher Throughput</a></h2>
            <div class="publication-date">2025-08-19 20:50</div>
            <div class="summary">🚀 Exciting news in AI! The NVIDIA Nemotron Nano 2 model has topped the leaderboard with impressive accuracy. This open reasoning model offers up to 6x higher throughput compared to its closest competitors, enhancing edge AI capabilities. Stay updated on advancements in technology! 🔍📈 #AI #NVIDIA #Innovation #EdgeComputing #Nemotron</div>
            <div class="source">Source: Nvidia Developer Blog</div>
            
            <div class="author">Chintan Patel</div>
            
            
            <div class="categories">
                
                <a href="/categories/product_announcements" class="category-text">Product Announcements</a>
                
            </div>
            
        </article>
        
    </div>
    
    
    <nav class="pagination">
        
        <a href="/sources/nvidia-developer-blog/" class="pagination-link">&laquo; Previous</a>
        
        
        <div class="pagination-numbers">
            
            
            <a href="/sources/nvidia-developer-blog/" class="pagination-link">1</a>
            
            
            
            <span class="pagination-current">2</span>
            
            
            
            <a href="/sources/nvidia-developer-blog/page/3/" class="pagination-link">3</a>
            
            
        </div>
        
        
        <a href="/sources/nvidia-developer-blog/page/3/" class="pagination-link">Next &raquo;</a>
        
    </nav>
    
</div>

    
    <footer class="site-footer">
        <div class="container">
            <p class="footer-text">
                Contents © 2025 <a href="#" id="author-email" class="author-link">Michał Rutkowski</a>
            </p>
        </div>
    </footer>
    <script>
        
        document.addEventListener('DOMContentLoaded', function() {
            function fixEmojiEncoding(text) {
                
                var fixed = text
                    
                    .replace(/ð���/g, '🚀')  
                    .replace(/ð��»/g, '👨‍💻')  
                    .replace(/ð���/g, '🎯')  
                    .replace(/ð��§/g, '🔧')  
                    .replace(/ð��¡/g, '💡')  
                    .replace(/ð���/g, '🔒')  
                    .replace(/ð��¡ï¸�/g, '🛡️')  
                    .replace(/ð���ï¸�/g, '⚗️')  
                    .replace(/ð��¬/g, '🚀')  
                    .replace(/ð���/g, '📊')  
                    .replace(/ð��/g, '📈')  
                    .replace(/ð�¤�/g, '🤖')  
                    
                    
                    .replace(/ð/g, '🚀')     
                    
                    
                    .replace(/�/g, '');     
                
                return fixed;
            }
            
            
            function fixAllText(element) {
                if (element.nodeType === Node.TEXT_NODE) {
                    var original = element.textContent;
                    
                    if (original.includes('ð') || original.includes('�')) {
                        var fixed = fixEmojiEncoding(original);
                        if (fixed !== original) {
                            element.textContent = fixed;
                        }
                    }
                } else {
                    for (var i = 0; i < element.childNodes.length; i++) {
                        fixAllText(element.childNodes[i]);
                    }
                }
            }
            
            
            fixAllText(document.body);
        });
        
        
        function toggleFilter(sectionId) {
            const content = document.getElementById(sectionId + '-content');
            const toggle = document.getElementById(sectionId + '-toggle');
            
            if (content.classList.contains('collapsed')) {
                content.classList.remove('collapsed');
                content.classList.add('expanded');
                toggle.textContent = '▲';
                toggle.style.transform = 'rotate(180deg)';
            } else {
                content.classList.remove('expanded');
                content.classList.add('collapsed');
                toggle.textContent = '▼';
                toggle.style.transform = 'rotate(0deg)';
            }
        }
        
        
        document.addEventListener('DOMContentLoaded', function() {
            function checkScreenSize() {
                const isMobile = window.innerWidth <= 768;
                const categoriesContent = document.getElementById('categories-content');
                const sourcesContent = document.getElementById('sources-content');
                const categoriesToggle = document.getElementById('categories-toggle');
                const sourcesToggle = document.getElementById('sources-toggle');
                
                if (categoriesContent && sourcesContent) {
                    if (isMobile) {
                        
                        categoriesContent.classList.add('collapsed');
                        categoriesContent.classList.remove('expanded');
                        sourcesContent.classList.add('collapsed');
                        sourcesContent.classList.remove('expanded');
                        categoriesToggle.textContent = '▼';
                        sourcesToggle.textContent = '▼';
                    } else {
                        
                        categoriesContent.classList.remove('collapsed');
                        categoriesContent.classList.add('expanded');
                        sourcesContent.classList.remove('collapsed');
                        sourcesContent.classList.add('expanded');
                        categoriesToggle.textContent = '▲';
                        sourcesToggle.textContent = '▲';
                    }
                }
            }
            
            
            checkScreenSize();
            window.addEventListener('resize', checkScreenSize);
            
            
            function cleanSourceNames() {
                const sourceLinks = document.querySelectorAll('.filter-options a[href*="/sources/"]');
                sourceLinks.forEach(link => {
                    if (link.textContent.includes('-Blog')) {
                        link.textContent = link.textContent.replace(/-Blog(\s*\(\d+\))?/g, '$1');
                    }
                });
            }
            
            
            cleanSourceNames();
            
            
            clearSourcesFilter();
            
            
            setupEmailLink();
        });
        
        
        function clearSourcesFilter() {
            const sourcesInput = document.getElementById('sources-filter');
            if (sourcesInput) {
                sourcesInput.value = '';
                
                filterSources();
            }
        }
        
        
        window.addEventListener('pageshow', function(event) {
            clearSourcesFilter();
        });
        
        
        function filterSources() {
            const input = document.getElementById('sources-filter');
            const filter = input.value.toLowerCase();
            const sourcesList = document.getElementById('sources-list');
            const links = sourcesList.getElementsByTagName('a');
            
            for (let i = 0; i < links.length; i++) {
                const link = links[i];
                const sourceName = link.getAttribute('data-source-name') || link.textContent;
                
                
                if (link.classList.contains('all-link')) {
                    continue;
                }
                
                if (sourceName.toLowerCase().indexOf(filter) > -1) {
                    link.style.display = '';
                } else {
                    link.style.display = 'none';
                }
            }
        }
        
        
        function setupEmailLink() {
            const emailLink = document.getElementById('author-email');
            if (emailLink) {
                
                const encodedEmail = 'aG9tZWVuZEB3cC5wbA==';
                
                
                const decodedEmail = atob(encodedEmail);
                const mailtoLink = 'mailto:' + decodedEmail;
                
                emailLink.href = mailtoLink;
                
                
                emailLink.addEventListener('click', function(e) {
                    
                });
            }
        }
    </script>
</body>
</html>
